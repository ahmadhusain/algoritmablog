---
title: 'Image Classification with Convolutional Network'
author: Arga
github: https://github.com/Argaadya/deep-learning/tree/master/image%20classification%20-%20cat_dog_panda
date: '2021-04-06'
slug: image-classification-cnn
categories:
  - R
tags:
  - Deep Learning
  - tidyverse
  - Machine Learning
  - Capstone ML
description: ''
featured: ''
featuredalt: ''
featuredpath: 'date'
linktitle: ''
type: post
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<style>
body {
text-align: justify}
</style>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Deep learning is a great approach to deal with unstructured data such as text, sound, video and image. There are a lot of implementation of deep learning in image classification and image detection, such as classifying image of dog or cats, detecting different objects in an image or do facial recognition.</p>
<center>
<img src="/img/image_class/train.gif" style="width:80.0%" />
</center>
<p>On this article, we will try to build a simple image classification that will classify whether the presented image is a cat, dog, or a panda.</p>
<center>
<img src="/img/image_class/grid.png" style="width:80.0%" />
</center>
</div>
<div id="data" class="section level1">
<h1>Data</h1>
<p>You can download the data and the source code for practice <a href="https://github.com/Argaadya/deep-learning/tree/master/image%20classification%20-%20cat_dog_panda">here</a>. The data is a modified version the <a href="https://www.kaggle.com/ashishsaxena2209/animal-image-datasetdog-cat-and-panda">Animal Image Dataset(DOG, CAT and PANDA)</a> on Kaggle.</p>
</div>
<div id="library-and-setup" class="section level1">
<h1>Library and Setup</h1>
<p>You need to install the <code>pillow</code> package in your conda environment to manipulate image data. Here is the short instruction on how to create a new conda environment with <code>tensorflow</code> and <code>pillow</code> inside it.</p>
<ol style="list-style-type: decimal">
<li>Open the terminal, either in anaconda command prompt or directly in RStudio.</li>
</ol>
<center>
<img src="/img/image_class/env.png" style="width:80.0%" />
</center>
<ol start="2" style="list-style-type: decimal">
<li>Create new conda environment by running the following command.</li>
</ol>
<p><code>conda create -n tf_image python=3.7</code></p>
<ol start="3" style="list-style-type: decimal">
<li>Active the conda environment by running the following command.</li>
</ol>
<p><code>conda activate tf_image</code></p>
<ol start="4" style="list-style-type: decimal">
<li>Install the tensorflow package into the environment.</li>
</ol>
<p><code>conda install -c conda-forge tensorflow=2</code></p>
<ol start="5" style="list-style-type: decimal">
<li>Install the <code>pillow</code> package.</li>
</ol>
<p><code>pip install pillow</code></p>
<ol start="6" style="list-style-type: decimal">
<li>The next step is just call your conda environment using <code>reticulate::use_python()</code> and insert the location of the python from the <code>tf_image</code> environment. You can locate the path or the location of the environment by typing <code>conda env list</code> in the terminal.</li>
</ol>
<center>
<img src="/img/image_class/env_list.png" style="width:80.0%" />
</center>
<pre class="r"><code># Use python in your anaconda3 environment folder
reticulate::use_python(&quot;~/anaconda3/envs/tf_image/bin/python&quot;, required = T)</code></pre>
<p>The following is the list of required packages to build and evaluate our image classification.</p>
<pre class="r"><code># Data wrangling
library(tidyverse)

# Image manipulation
library(imager)

# Deep learning
library(keras)

# Model Evaluation
library(caret)

options(scipen = 999)</code></pre>
</div>
<div id="exploratory-data-analysis" class="section level1">
<h1>Exploratory Data Analysis</h1>
<p>Let’s explore the data first before building the model. In image classification problem, it is a common practice to put each image on separate folders based on the target class/labels. For example, inside the train folder in our data, you can that we have 3 different folders, respectively for <code>cats</code>, <code>dogs</code>, and <code>panda</code>.</p>
<center>
<img src="/img/image_class/folder.png" style="width:80.0%" />
</center>
<p>If you open the cat folder, you can see that we have no table or any kind of structured data format, we only have the image for the cat. We will directly extract information from the images instead of using a structured dataset.</p>
<center>
<img src="/img/image_class/cat_image.png" style="width:80.0%" />
</center>
<p>Let’s try to get the file name of each image. First, we need to locate the folder of each target class. The following code will give you the folder name inside the <code>train</code> folder.</p>
<pre class="r"><code>folder_list &lt;- list.files(&quot;data_input/image_class/train/&quot;)

folder_list</code></pre>
<pre><code>#&gt; [1] &quot;cats&quot;  &quot;dogs&quot;  &quot;panda&quot;</code></pre>
<p>We combine the folder name with the path or directory of the <code>train</code> folder in order to access the content inside each folder.</p>
<pre class="r"><code>folder_path &lt;- paste0(&quot;data_input/image_class/train/&quot;, folder_list, &quot;/&quot;)

folder_path</code></pre>
<pre><code>#&gt; [1] &quot;data_input/image_class/train/cats/&quot;  &quot;data_input/image_class/train/dogs/&quot; 
#&gt; [3] &quot;data_input/image_class/train/panda/&quot;</code></pre>
<p>We will use the <code>map()</code> function to loop or iterate and collect the file name for each folder (cat, dog, panda). The <code>map()</code> will return a list so if we want to combine the file name from 3 different folders we simply use the <code>unlist()</code> function.</p>
<pre class="r"><code># Get file name
file_name &lt;- map(folder_path, 
                 function(x) paste0(x, list.files(x))
                 ) %&gt;% 
  unlist()

# first 6 file name
head(file_name)</code></pre>
<pre><code>#&gt; [1] &quot;data_input/image_class/train/cats/cats_001.jpg&quot;
#&gt; [2] &quot;data_input/image_class/train/cats/cats_002.jpg&quot;
#&gt; [3] &quot;data_input/image_class/train/cats/cats_003.jpg&quot;
#&gt; [4] &quot;data_input/image_class/train/cats/cats_004.jpg&quot;
#&gt; [5] &quot;data_input/image_class/train/cats/cats_005.jpg&quot;
#&gt; [6] &quot;data_input/image_class/train/cats/cats_006.jpg&quot;</code></pre>
<p>You can also check the last 6 images.</p>
<pre class="r"><code># last 6 file name
tail(file_name)</code></pre>
<pre><code>#&gt; [1] &quot;data_input/image_class/train/panda/panda_00994.jpg&quot;
#&gt; [2] &quot;data_input/image_class/train/panda/panda_00995.jpg&quot;
#&gt; [3] &quot;data_input/image_class/train/panda/panda_00996.jpg&quot;
#&gt; [4] &quot;data_input/image_class/train/panda/panda_00997.jpg&quot;
#&gt; [5] &quot;data_input/image_class/train/panda/panda_00998.jpg&quot;
#&gt; [6] &quot;data_input/image_class/train/panda/panda_00999.jpg&quot;</code></pre>
<p>Let’s check how many images we have.</p>
<pre class="r"><code>length(file_name)</code></pre>
<pre><code>#&gt; [1] 2659</code></pre>
<p>To check the content of the file, we can use the <code>load.image()</code> function from the <code>imager</code> package. For example, let’s randomly visualize 6 images from the data.</p>
<pre class="r"><code># Randomly select image
set.seed(99)
sample_image &lt;- sample(file_name, 6)

# Load image into R
img &lt;- map(sample_image, load.image)

# Plot image
par(mfrow = c(2, 3)) # Create 2 x 3 image grid
map(img, plot)</code></pre>
<p><img src="/blog/2021-03-21-image-classification-with-cnn_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>#&gt; [[1]]
#&gt; Image. Width: 184 pix Height: 149 pix Depth: 1 Colour channels: 3 
#&gt; 
#&gt; [[2]]
#&gt; Image. Width: 500 pix Height: 375 pix Depth: 1 Colour channels: 3 
#&gt; 
#&gt; [[3]]
#&gt; Image. Width: 500 pix Height: 375 pix Depth: 1 Colour channels: 3 
#&gt; 
#&gt; [[4]]
#&gt; Image. Width: 500 pix Height: 466 pix Depth: 1 Colour channels: 3 
#&gt; 
#&gt; [[5]]
#&gt; Image. Width: 499 pix Height: 375 pix Depth: 1 Colour channels: 3 
#&gt; 
#&gt; [[6]]
#&gt; Image. Width: 500 pix Height: 397 pix Depth: 1 Colour channels: 3</code></pre>
<div id="check-image-dimension" class="section level2">
<h2>Check Image Dimension</h2>
<p>One of important aspects of image classification is understand the dimension of the input images. You need to know the distribution of the image dimension to create a proper input dimension for building the deep learning model. Let’s check the properties of the first image.</p>
<pre class="r"><code># Full Image Description
img &lt;- load.image(file_name[1])
img</code></pre>
<pre><code>#&gt; Image. Width: 499 pix Height: 375 pix Depth: 1 Colour channels: 3</code></pre>
<p>You can get the information about the dimension of the image. The height and width represent the height and width of the image in pixels. The color channel represent if the color is in grayscale format (color channels = 1) or is in RGB format (color channels = 3). To get the value of each dimension, we can use the <code>dim()</code> function. It will return the height, width, depth, and the channels.</p>
<pre class="r"><code># Image Dimension
dim(img)</code></pre>
<pre><code>#&gt; [1] 499 375   1   3</code></pre>
<p>So we have successfully insert an image and get the image dimensions. On the following code, we will create a function that will instantly get the height and width of an image and convert it into a <code>data.frame</code>.</p>
<pre class="r"><code># Function for acquiring width and height of an image
get_dim &lt;- function(x){
  img &lt;- load.image(x) 
  
  df_img &lt;- data.frame(height = height(img),
                       width = width(img),
                       filename = x
                       )
  
  return(df_img)
}

get_dim(file_name[1])</code></pre>
<pre><code>#&gt;   height width                                       filename
#&gt; 1    375   499 data_input/image_class/train/cats/cats_001.jpg</code></pre>
<p>Now we will sampling 1,000 images from the file name and get the height and width of the image. We use sampling here because it will take a quite long time to load all images.</p>
<pre class="r"><code># Randomly get 1000 sample images
set.seed(123)
sample_file &lt;- sample(file_name, 1000)

# Run the get_dim() function for each image
file_dim &lt;- map_df(sample_file, get_dim)

head(file_dim, 10)</code></pre>
<pre><code>#&gt;    height width                                           filename
#&gt; 1     375   500 data_input/image_class/train/panda/panda_00780.jpg
#&gt; 2     375   500 data_input/image_class/train/panda/panda_00836.jpg
#&gt; 3     375   500 data_input/image_class/train/panda/panda_00521.jpg
#&gt; 4     294   363     data_input/image_class/train/cats/cats_526.jpg
#&gt; 5     269   172     data_input/image_class/train/cats/cats_195.jpg
#&gt; 6     375   500 data_input/image_class/train/panda/panda_00072.jpg
#&gt; 7     255   234   data_input/image_class/train/dogs/dogs_00313.jpg
#&gt; 8     499   375   data_input/image_class/train/dogs/dogs_00450.jpg
#&gt; 9     374   500   data_input/image_class/train/dogs/dogs_00466.jpg
#&gt; 10    500   440   data_input/image_class/train/dogs/dogs_00187.jpg</code></pre>
<p>Now let’s get the statistics for the image dimensions.</p>
<pre class="r"><code>summary(file_dim)</code></pre>
<pre><code>#&gt;      height           width          filename        
#&gt;  Min.   :  50.0   Min.   :  59.0   Length:1000       
#&gt;  1st Qu.: 331.0   1st Qu.: 350.0   Class :character  
#&gt;  Median : 375.0   Median : 499.0   Mode  :character  
#&gt;  Mean   : 368.2   Mean   : 425.4                     
#&gt;  3rd Qu.: 411.2   3rd Qu.: 500.0                     
#&gt;  Max.   :1023.0   Max.   :1024.0</code></pre>
<p>The image data has a great variation in the dimension. Some images has less than 60 pixels in height and width while others has up to 1024 pixels. Understanding the dimension of the image will help us on the next part of the process: data preprocessing.</p>
</div>
</div>
<div id="data-preprocessing" class="section level1">
<h1>Data Preprocessing</h1>
<p>Data preprocessing for image is pretty simple and can be done in a single step in the following section.</p>
<div id="data-augmentation" class="section level2">
<h2>Data Augmentation</h2>
<p>Based on our previous summary of the image dimensions, we can determine the input dimension for the deep learning model. All input images should have the same dimensions. Here, we can determine the input size for the image, for example transform all image into 64 x 64 pixels. This process will be similar to us resizing the image. You can use other choice of image dimensions, such as 125 x 125 pixels or even 200 x 200 pixels. Bigger dimensions will have more features but will also take longer time to train. However, if the image size is too small, we will lose a lot of information from the data. So balancing this trade-off is the art of data preprocessing in image classification.</p>
<p>We also set the batch size for the data so the model will be updated every time it finished training on a single batch. Here, we set the batch size to 32.</p>
<pre class="r"><code># Desired height and width of images
target_size &lt;- c(64, 64)

# Batch size for training the model
batch_size &lt;- 32</code></pre>
<p>Since we have a little amount of training set, we will build artificial data using method called <code>Image Augmentation</code>. Image augmentation is one useful technique in building models that can increase the size of the training set without acquiring new images. The goal is that to teach the model not only with the original image but also the modification of the image, such as flipping the image, rotate it, zooming, crop the image, etc. This will create more robust model. We can do data augmentation by using the image data generator from <code>keras</code>.</p>
<center>
<img src="/img/image_class/augmentation.jpg" style="width:80.0%" />
</center>
<p>To do image augmentation, we can fit the data into a <code>generator</code>. Here, we will create the image generator for keras with the following properties:</p>
<ul>
<li>Scaling the pixel value by dividing the pixel value by 255</li>
<li>Flip the image horizontally</li>
<li>Flip the image vertically</li>
<li>Rotate the image from 0 to 45 degrees</li>
<li>Zoom in or zoom out by 25% (zoom 75% or 125%)</li>
<li>Use 20% of the data as validation dataset</li>
</ul>
<p>You can explore more features about the image generator on <a href="https://tensorflow.rstudio.com/reference/keras/image_data_generator/">this link</a>.</p>
<pre class="r"><code># Image Generator
train_data_gen &lt;- image_data_generator(rescale = 1/255, # Scaling pixel value
                                       horizontal_flip = T, # Flip image horizontally
                                       vertical_flip = T, # Flip image vertically 
                                       rotation_range = 45, # Rotate image from 0 to 45 degrees
                                       zoom_range = 0.25, # Zoom in or zoom out range
                                       validation_split = 0.2 # 20% data as validation data
                                       )</code></pre>
<p>Now we can insert our image data into the generator using the <code>flow_images_from_directory()</code>. The data is located inside the <code>data</code> folder and inside the <code>train</code> folder, so the directory will be <code>data/train</code>. From this process, we will get the augmented image both for training data and the validation data.</p>
<pre class="r"><code># Training Dataset
train_image_array_gen &lt;- flow_images_from_directory(directory = &quot;data_input/image_class/train/&quot;, # Folder of the data
                                                    target_size = target_size, # target of the image dimension (64 x 64)  
                                                    color_mode = &quot;rgb&quot;, # use RGB color
                                                    batch_size = batch_size , 
                                                    seed = 123,  # set random seed
                                                    subset = &quot;training&quot;, # declare that this is for training data
                                                    generator = train_data_gen
                                                    )

# Validation Dataset
val_image_array_gen &lt;- flow_images_from_directory(directory = &quot;data_input/image_class/train/&quot;,
                                                  target_size = target_size, 
                                                  color_mode = &quot;rgb&quot;, 
                                                  batch_size = batch_size ,
                                                  seed = 123,
                                                  subset = &quot;validation&quot;, # declare that this is the validation data
                                                  generator = train_data_gen
                                                  )</code></pre>
<p>Here we will collect some information from the generator and check the class proportion of the train dataset. The index correspond to each labels of the target variable and ordered alphabetically (cat, dog, panda).</p>
<pre class="r"><code># Number of training samples
train_samples &lt;- train_image_array_gen$n

# Number of validation samples
valid_samples &lt;- val_image_array_gen$n

# Number of target classes/categories
output_n &lt;- n_distinct(train_image_array_gen$classes)

# Get the class proportion
table(&quot;\nFrequency&quot; = factor(train_image_array_gen$classes)
      ) %&gt;% 
  prop.table()</code></pre>
<pre><code>#&gt; 
#&gt; Frequency
#&gt;         0         1         2 
#&gt; 0.3344293 0.3363081 0.3292626</code></pre>
</div>
</div>
<div id="convolutional-neural-network" class="section level1">
<h1>Convolutional Neural Network</h1>
<p>The <code>Convolutional Neural Network</code> or <code>Convolutional Layer</code> is a popular layer for image classification. If you remember, an image is just a 2 dimensional array with certain height and width. For example, an image with 64 x 64 pixels means that it has 4096 pixels that is distributed in a 64 x 64 array instead of a single dimensional vector. The benefit of using image as a 2D array is that we can extract certain features from the image such as the shape of nose, the shape of eyes, hand, etc.</p>
<p>Take the following amazing example from <a href="https://setosa.io/ev/image-kernels/">setosa.io</a>. We have an image and its 2D array representation. The value on the array is the pixel value from the image, higher value means brighter pixel.</p>
<center>
<img src="/img/image_class/image_array.png" style="width:80.0%" />
</center>
<p>To extract features from the image, we create something called a <code>filter kernel</code>. A filter is an array with certain size, for example 3 x 3 array to capture features from image. In the following figure, the rectangle illustrated a single filter kernel.</p>
<center>
<img src="/img/image_class/architecture-cnn-en.jpeg" style="width:80.0%" />
</center>
<p>For example, the left side of the following picture is a 5 x 5 images. The kernel has a weight that will capture certain features, which in this example is an <code>X</code> features that indicated by the value of 1 create an X shape. The new convoluted feature is the product of the image section with the kernel feature. The more similar image section with the kernel, the higher the score of the convoluted feature.</p>
<center>
<img src="/img/image_class/kernel.png" style="width:80.0%" />
</center>
<p>The kernel wil move sideway to the right to capture each section of the image to create new convoluted feature. If the kerel has reach the edge, it will go down 1 row below and continue the process. The process is illustrated as follows. Visit stanford course on <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks">Convolutional Neural Network</a> for more info about the figure and other info.</p>
<center>
<img src="/img/image_class/cnn.png" style="width:80.0%" />
</center>
<p>To highglight the most important feature and also downsize the dimension of the convoluted feature, we can use a method called the <code>Max pooling</code> which take only the maximum value from certain window. For example, in the top left array that contains the value of 1, 1, 5, and 6, it only take the max value, which is 6.</p>
<center>
<img src="/img/image_class/max_pool2.png" style="width:80.0%" />
</center>
<p>Below is the illustration of doing max pooling of 2 x 2 pooling area along the sections. Max pooling only take the maximum value of each pooling area.</p>
<center>
<img src="/img/image_class/max-pooling-a.png" style="width:80.0%" />
</center>
<p>To make the extracted 2D array into a 1D array, we use the flattening layer so we can continue using the fully-connected dense layer and to the output layer.</p>
<center>
<img src="/img/image_class/fully-connected-ltr.png" style="width:80.0%" />
</center>
<p>The following figure illustrate the full deep learning model with CNN, max pooling and fully connected dense layer.</p>
<center>
<img src="/img/image_class/cnn-deep.jpeg" style="width:80.0%" />
</center>
<p>You can see the explanation from MIT for <a href="https://www.youtube.com/watch?v=iaSUYvmCekI&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=4&amp;t=0s">introduction to Deep Learning and Convolutional Neural Network</a> on the deeper level.</p>
</div>
<div id="model-architecture" class="section level1">
<h1>Model Architecture</h1>
<p>We can start building the model architecture for the deep learning. We will build a simple model first with the following layer:</p>
<ul>
<li>Convolutional layer to extract features from 2D image with <code>relu</code> activation function</li>
<li>Max Pooling layer to downsample the image features</li>
<li>Flattening layer to flatten data from 2D array to 1D array</li>
<li>Dense layer to capture more information</li>
<li>Dense layer for output with <code>softmax</code> activation function</li>
</ul>
<p>Don’t forget to set the input size in the first layer. If the input image is in <code>RGB</code>, set the final number to 3, which is the number of color channels. If the input image is in <code>grayscale</code>, set the final number to 1.</p>
<pre class="r"><code># input shape of the image
c(target_size, 3) </code></pre>
<pre><code>#&gt; [1] 64 64  3</code></pre>
<pre class="r"><code># Set Initial Random Weight
tensorflow::tf$random$set_seed(123)

model &lt;- keras_model_sequential(name = &quot;simple_model&quot;) %&gt;% 
  
  # Convolution Layer
  layer_conv_2d(filters = 16,
                kernel_size = c(3,3),
                padding = &quot;same&quot;,
                activation = &quot;relu&quot;,
                input_shape = c(target_size, 3) 
                ) %&gt;% 

  # Max Pooling Layer
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% 
  
  # Flattening Layer
  layer_flatten() %&gt;% 
  
  # Dense Layer
  layer_dense(units = 16,
              activation = &quot;relu&quot;) %&gt;% 
  
  # Output Layer
  layer_dense(units = output_n,
              activation = &quot;softmax&quot;,
              name = &quot;Output&quot;)
  
model</code></pre>
<pre><code>#&gt; Model
#&gt; Model: &quot;simple_model&quot;
#&gt; ________________________________________________________________________________
#&gt; Layer (type)                        Output Shape                    Param #     
#&gt; ================================================================================
#&gt; conv2d (Conv2D)                     (None, 64, 64, 16)              448         
#&gt; ________________________________________________________________________________
#&gt; max_pooling2d (MaxPooling2D)        (None, 32, 32, 16)              0           
#&gt; ________________________________________________________________________________
#&gt; flatten (Flatten)                   (None, 16384)                   0           
#&gt; ________________________________________________________________________________
#&gt; dense (Dense)                       (None, 16)                      262160      
#&gt; ________________________________________________________________________________
#&gt; Output (Dense)                      (None, 3)                       51          
#&gt; ================================================================================
#&gt; Total params: 262,659
#&gt; Trainable params: 262,659
#&gt; Non-trainable params: 0
#&gt; ________________________________________________________________________________</code></pre>
<p>As you can see, we start by entering image data with 64 x 64 pixels into the convolutional layer, which has 16 filters to extract featuers from the image. The <code>padding = same</code> argument is used to keep the dimension of the feature to be 64 x 64 pixels after being extracted. We then downsample or only take the maximum value for each 2x2 pooling area so the data now only has 32 x 32 pixels with from 16 filters. After that, from 32 x 32 pixels we flatten the 2D array into a 1D array with 32 x 32 x 16 = 16384 nodes. We can further extract information using the simple dense layer and finished by flowing the information into the output layer, which will be transformed using the <code>softmax</code> activation function to get the probability of each class as the output.</p>
</div>
<div id="model-fitting" class="section level1">
<h1>Model Fitting</h1>
<p>You can start fitting the data into the model. Don’t forget to compile the model by specifying the loss function and the optimizer. For starter, we will use 30 epochs to train the data. For multilabel classification, we will use <code>categorical cross-entropy</code> as the loss function. For this example, we use <code>adam</code> optimizer with learning rate of 0.01. We will also evaluate the model with the validation data from the generator.</p>
<pre class="r"><code>model %&gt;% 
  compile(
    loss = &quot;categorical_crossentropy&quot;,
    optimizer = optimizer_adam(lr = 0.01),
    metrics = &quot;accuracy&quot;
  )

# Fit data into model
history &lt;- model %&gt;% 
  fit(
  # training data
  train_image_array_gen,

  # training epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = 30, 
  
  # validation data
  validation_data = val_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size)
)

plot(history)</code></pre>
<p><img src="/blog/2021-03-21-image-classification-with-cnn_files/figure-html/unnamed-chunk-21-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="model-evaluation" class="section level1">
<h1>Model Evaluation</h1>
<p>Now we will further evaluate and acquire the confusion matrix using the validation data from the generator. First, we need to acquire the file name of the image that is used as the data validation. From the file name, we will extract the categorical label as the actual value of the target variable.</p>
<pre class="r"><code>val_data &lt;- data.frame(file_name = paste0(&quot;data_input/image_class/train/&quot;, val_image_array_gen$filenames)) %&gt;% 
  mutate(class = str_extract(file_name, &quot;cat|dog|panda&quot;))

head(val_data, 10)</code></pre>
<pre><code>#&gt;                                         file_name class
#&gt; 1  data_input/image_class/train/cats/cats_001.jpg   cat
#&gt; 2  data_input/image_class/train/cats/cats_002.jpg   cat
#&gt; 3  data_input/image_class/train/cats/cats_003.jpg   cat
#&gt; 4  data_input/image_class/train/cats/cats_004.jpg   cat
#&gt; 5  data_input/image_class/train/cats/cats_005.jpg   cat
#&gt; 6  data_input/image_class/train/cats/cats_006.jpg   cat
#&gt; 7  data_input/image_class/train/cats/cats_007.jpg   cat
#&gt; 8  data_input/image_class/train/cats/cats_008.jpg   cat
#&gt; 9  data_input/image_class/train/cats/cats_009.jpg   cat
#&gt; 10 data_input/image_class/train/cats/cats_010.jpg   cat</code></pre>
<p>What to do next? We need to get the image into R by converting the image into an array. Since our input dimension for CNN model is image with 64 x 64 pixels with 3 color channels (RGB), we will do the same with the image of the testing data. The reason of using array is that we want to predict the original image fresh from the folder so we will not use the image generator since it will transform the image and does not reflect the actual image.</p>
<pre class="r"><code># Function to convert image to array
image_prep &lt;- function(x) {
  arrays &lt;- lapply(x, function(path) {
    img &lt;- image_load(path, target_size = target_size, 
                      grayscale = F # Set FALSE if image is RGB
                      )
    
    x &lt;- image_to_array(img)
    x &lt;- array_reshape(x, c(1, dim(x)))
    x &lt;- x/255 # rescale image pixel
  })
  do.call(abind::abind, c(arrays, list(along = 1)))
}</code></pre>
<pre class="r"><code>test_x &lt;- image_prep(val_data$file_name)

# Check dimension of testing data set
dim(test_x)</code></pre>
<pre><code>#&gt; [1] 530  64  64   3</code></pre>
<p>The validation data consists of 530 images with dimensions of 64 x 64 pixels and 3 color channels (RGB). After we have prepared the data test, we now can proceed to predict the label of each image using our CNN model.</p>
<pre class="r"><code>pred_test &lt;- predict_classes(model, test_x) 

head(pred_test, 10)</code></pre>
<pre><code>#&gt;  [1] 1 0 0 2 1 2 0 0 0 0</code></pre>
<p>To get easier interpretation of the prediction, we will convert the encoding into proper class label.</p>
<pre class="r"><code># Convert encoding to label
decode &lt;- function(x){
  case_when(x == 0 ~ &quot;cat&quot;,
            x == 1 ~ &quot;dog&quot;,
            x == 2 ~ &quot;panda&quot;
            )
}

pred_test &lt;- sapply(pred_test, decode) 

head(pred_test, 10)</code></pre>
<pre><code>#&gt;  [1] &quot;dog&quot;   &quot;cat&quot;   &quot;cat&quot;   &quot;panda&quot; &quot;dog&quot;   &quot;panda&quot; &quot;cat&quot;   &quot;cat&quot;   &quot;cat&quot;  
#&gt; [10] &quot;cat&quot;</code></pre>
<p>Finally, we evaluate the model using the confusion matrix. The model perform very poorly with low accuracy. We will tune the model by improving the model architecture.</p>
<pre class="r"><code>confusionMatrix(as.factor(pred_test), 
                as.factor(val_data$class)
                )</code></pre>
<pre><code>#&gt; Confusion Matrix and Statistics
#&gt; 
#&gt;           Reference
#&gt; Prediction cat dog panda
#&gt;      cat    65  48     1
#&gt;      dog    42  51     7
#&gt;      panda  70  79   167
#&gt; 
#&gt; Overall Statistics
#&gt;                                                
#&gt;                Accuracy : 0.534                
#&gt;                  95% CI : (0.4905, 0.5771)     
#&gt;     No Information Rate : 0.3358               
#&gt;     P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022
#&gt;                                                
#&gt;                   Kappa : 0.3023               
#&gt;                                                
#&gt;  Mcnemar&#39;s Test P-Value : &lt; 0.00000000000000022
#&gt; 
#&gt; Statistics by Class:
#&gt; 
#&gt;                      Class: cat Class: dog Class: panda
#&gt; Sensitivity              0.3672    0.28652       0.9543
#&gt; Specificity              0.8612    0.86080       0.5803
#&gt; Pos Pred Value           0.5702    0.51000       0.5285
#&gt; Neg Pred Value           0.7308    0.70465       0.9626
#&gt; Prevalence               0.3340    0.33585       0.3302
#&gt; Detection Rate           0.1226    0.09623       0.3151
#&gt; Detection Prevalence     0.2151    0.18868       0.5962
#&gt; Balanced Accuracy        0.6142    0.57366       0.7673</code></pre>
</div>
<div id="tuning-the-model" class="section level1">
<h1>Tuning the Model</h1>
<div id="model-architecture-1" class="section level2">
<h2>Model Architecture</h2>
<p>Let’s look back at our model architecture. If you have noticed, we can actually extract more information while the data is still in an 2D image array. The first CNN only extract the general features of our image and then being downsampled using the max pooling layer. Even after pooling, we still have 32 x 32 array that has a lot of information to extract before flattening the data. Therefore, we can stacks more CNN layers into the model so there will be more information to be captured. We can also put 2 CNN layers consecutively before doing max pooling.</p>
<pre class="r"><code>model</code></pre>
<pre><code>#&gt; Model
#&gt; Model: &quot;simple_model&quot;
#&gt; ________________________________________________________________________________
#&gt; Layer (type)                        Output Shape                    Param #     
#&gt; ================================================================================
#&gt; conv2d (Conv2D)                     (None, 64, 64, 16)              448         
#&gt; ________________________________________________________________________________
#&gt; max_pooling2d (MaxPooling2D)        (None, 32, 32, 16)              0           
#&gt; ________________________________________________________________________________
#&gt; flatten (Flatten)                   (None, 16384)                   0           
#&gt; ________________________________________________________________________________
#&gt; dense (Dense)                       (None, 16)                      262160      
#&gt; ________________________________________________________________________________
#&gt; Output (Dense)                      (None, 3)                       51          
#&gt; ================================================================================
#&gt; Total params: 262,659
#&gt; Trainable params: 262,659
#&gt; Non-trainable params: 0
#&gt; ________________________________________________________________________________</code></pre>
<p>The following is our improved model architecture:</p>
<ul>
<li>1st Convolutional layer to extract features from 2D image with <code>relu</code> activation function</li>
<li>2nd Convolutional layer to extract features from 2D image with <code>relu</code> activation function</li>
<li>Max pooling layer</li>
<li>3rd Convolutional layer to extract features from 2D image with <code>relu</code> activation function</li>
<li>Max pooling layer</li>
<li>4th Convolutional layer to extract features from 2D image with <code>relu</code> activation function</li>
<li>Max pooling layer</li>
<li>5th Convolutional layer to extract features from 2D image with <code>relu</code> activation function</li>
<li>Max pooling layer</li>
<li>Flattening layer from 2D array to 1D array</li>
<li>Dense layer to capture more information</li>
<li>Dense layer for output layer</li>
</ul>
<p>You can play and get creative by designing your own model architecture.</p>
<pre class="r"><code>tensorflow::tf$random$set_seed(123)

model_big &lt;- keras_model_sequential() %&gt;% 
  
  # First convolutional layer
  layer_conv_2d(filters = 32,
                kernel_size = c(5,5), # 5 x 5 filters
                padding = &quot;same&quot;,
                activation = &quot;relu&quot;,
                input_shape = c(target_size, 3)
                ) %&gt;% 
  
  # Second convolutional layer
  layer_conv_2d(filters = 32,
                kernel_size = c(3,3), # 3 x 3 filters
                padding = &quot;same&quot;,
                activation = &quot;relu&quot;
                ) %&gt;% 
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% 
  
  # Third convolutional layer
  layer_conv_2d(filters = 64,
                kernel_size = c(3,3),
                padding = &quot;same&quot;,
                activation = &quot;relu&quot;
                ) %&gt;% 

  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% 
  
  # Fourth convolutional layer
  layer_conv_2d(filters = 128,
                kernel_size = c(3,3),
                padding = &quot;same&quot;,
                activation = &quot;relu&quot;
                ) %&gt;% 
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% 

  # Fifth convolutional layer
  layer_conv_2d(filters = 256,
                kernel_size = c(3,3),
                padding = &quot;same&quot;,
                activation = &quot;relu&quot;
                ) %&gt;% 
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% 
  
  # Flattening layer
  layer_flatten() %&gt;% 
  
  # Dense layer
  layer_dense(units = 64,
              activation = &quot;relu&quot;) %&gt;% 
  
  # Output layer
  layer_dense(name = &quot;Output&quot;,
              units = 3, 
              activation = &quot;softmax&quot;)

model_big</code></pre>
<pre><code>#&gt; Model
#&gt; Model: &quot;sequential&quot;
#&gt; ________________________________________________________________________________
#&gt; Layer (type)                        Output Shape                    Param #     
#&gt; ================================================================================
#&gt; conv2d_5 (Conv2D)                   (None, 64, 64, 32)              2432        
#&gt; ________________________________________________________________________________
#&gt; conv2d_4 (Conv2D)                   (None, 64, 64, 32)              9248        
#&gt; ________________________________________________________________________________
#&gt; max_pooling2d_4 (MaxPooling2D)      (None, 32, 32, 32)              0           
#&gt; ________________________________________________________________________________
#&gt; conv2d_3 (Conv2D)                   (None, 32, 32, 64)              18496       
#&gt; ________________________________________________________________________________
#&gt; max_pooling2d_3 (MaxPooling2D)      (None, 16, 16, 64)              0           
#&gt; ________________________________________________________________________________
#&gt; conv2d_2 (Conv2D)                   (None, 16, 16, 128)             73856       
#&gt; ________________________________________________________________________________
#&gt; max_pooling2d_2 (MaxPooling2D)      (None, 8, 8, 128)               0           
#&gt; ________________________________________________________________________________
#&gt; conv2d_1 (Conv2D)                   (None, 8, 8, 256)               295168      
#&gt; ________________________________________________________________________________
#&gt; max_pooling2d_1 (MaxPooling2D)      (None, 4, 4, 256)               0           
#&gt; ________________________________________________________________________________
#&gt; flatten_1 (Flatten)                 (None, 4096)                    0           
#&gt; ________________________________________________________________________________
#&gt; dense_1 (Dense)                     (None, 64)                      262208      
#&gt; ________________________________________________________________________________
#&gt; Output (Dense)                      (None, 3)                       195         
#&gt; ================================================================================
#&gt; Total params: 661,603
#&gt; Trainable params: 661,603
#&gt; Non-trainable params: 0
#&gt; ________________________________________________________________________________</code></pre>
</div>
<div id="model-fitting-1" class="section level2">
<h2>Model Fitting</h2>
<p>We can once again fit the model into the data. We will let the data train with more epochs since we have small numbers of data. For example, we will train the data with 50 epochs. We will also lower the learning rate from 0.01 to 0.001.</p>
<pre class="r"><code>model_big %&gt;% 
  compile(
    loss = &quot;categorical_crossentropy&quot;,
    optimizer = optimizer_adam(lr = 0.001),
    metrics = &quot;accuracy&quot;
  )

history &lt;- model %&gt;% 
  fit_generator(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = 50, 
  
  # validation data
  validation_data = val_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # print progress but don&#39;t create graphic
  verbose = 1,
  view_metrics = 0
)

plot(history)</code></pre>
<p><img src="/blog/2021-03-21-image-classification-with-cnn_files/figure-html/unnamed-chunk-32-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="model-evaluation-1" class="section level2">
<h2>Model Evaluation</h2>
<p>Now we will further evaluate the data and acquire the confusion matrix for the validation data.</p>
<pre class="r"><code>pred_test &lt;- predict_classes(model_big, test_x) 

head(pred_test, 10)</code></pre>
<pre><code>#&gt;  [1] 1 0 0 1 0 2 0 0 0 0</code></pre>
<p>To get easier interpretation of the prediction, we will convert the encoding into proper class label.</p>
<pre class="r"><code># Convert encoding to label
decode &lt;- function(x){
  case_when(x == 0 ~ &quot;cat&quot;,
            x == 1 ~ &quot;dog&quot;,
            x == 2 ~ &quot;panda&quot;
            )
}

pred_test &lt;- sapply(pred_test, decode) 

head(pred_test, 10)</code></pre>
<pre><code>#&gt;  [1] &quot;dog&quot;   &quot;cat&quot;   &quot;cat&quot;   &quot;dog&quot;   &quot;cat&quot;   &quot;panda&quot; &quot;cat&quot;   &quot;cat&quot;   &quot;cat&quot;  
#&gt; [10] &quot;cat&quot;</code></pre>
<p>Finally, we evaluate the model using the confusion matrix. This model perform better than the previous model because we put more CNN layer to extract more features from the image.</p>
<pre class="r"><code>confusionMatrix(as.factor(pred_test), 
                as.factor(val_data$class)
                )</code></pre>
<pre><code>#&gt; Confusion Matrix and Statistics
#&gt; 
#&gt;           Reference
#&gt; Prediction cat dog panda
#&gt;      cat   122  54     5
#&gt;      dog    43 100     8
#&gt;      panda  12  24   162
#&gt; 
#&gt; Overall Statistics
#&gt;                                                
#&gt;                Accuracy : 0.7245               
#&gt;                  95% CI : (0.6844, 0.7622)     
#&gt;     No Information Rate : 0.3358               
#&gt;     P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022
#&gt;                                                
#&gt;                   Kappa : 0.5869               
#&gt;                                                
#&gt;  Mcnemar&#39;s Test P-Value : 0.006952             
#&gt; 
#&gt; Statistics by Class:
#&gt; 
#&gt;                      Class: cat Class: dog Class: panda
#&gt; Sensitivity              0.6893     0.5618       0.9257
#&gt; Specificity              0.8329     0.8551       0.8986
#&gt; Pos Pred Value           0.6740     0.6623       0.8182
#&gt; Neg Pred Value           0.8424     0.7942       0.9608
#&gt; Prevalence               0.3340     0.3358       0.3302
#&gt; Detection Rate           0.2302     0.1887       0.3057
#&gt; Detection Prevalence     0.3415     0.2849       0.3736
#&gt; Balanced Accuracy        0.7611     0.7085       0.9122</code></pre>
</div>
</div>
<div id="predict-data-in-testing-dataset" class="section level1">
<h1>Predict Data in Testing Dataset</h1>
<p>After we have trained the model and if you have satisfied with the model performance on the validation dataset, we will do another model evaluation using the testing dataset. The testing data is located on the <code>test</code> folder.</p>
<pre class="r"><code>df_test  &lt;- read.csv(&quot;data_input/image_class/metadata_test.csv&quot;)

head(df_test, 10)</code></pre>
<pre><code>#&gt;    class                              file_name
#&gt; 1    cat data_input/image_class/test/img_01.jpg
#&gt; 2    cat data_input/image_class/test/img_02.jpg
#&gt; 3    cat data_input/image_class/test/img_03.jpg
#&gt; 4    cat data_input/image_class/test/img_04.jpg
#&gt; 5    cat data_input/image_class/test/img_05.jpg
#&gt; 6    cat data_input/image_class/test/img_06.jpg
#&gt; 7    cat data_input/image_class/test/img_07.jpg
#&gt; 8    cat data_input/image_class/test/img_08.jpg
#&gt; 9    cat data_input/image_class/test/img_09.jpg
#&gt; 10   cat data_input/image_class/test/img_10.jpg</code></pre>
<p>Then, we convert the image into 2D array.</p>
<pre class="r"><code>test_x &lt;- image_prep(df_test$file_name)

# Check dimension of testing data set
dim(test_x)</code></pre>
<pre><code>#&gt; [1] 340  64  64   3</code></pre>
<p>The testing data consists of 341 images with dimension of 64 x 64 pixels and 3 color channels (RGB). After we have prepared the data test, we now can proceed to predict the label of each image using our CNN model.</p>
<pre class="r"><code>pred_test &lt;- predict_classes(model_big, test_x) 

head(pred_test, 10)</code></pre>
<pre><code>#&gt;  [1] 0 0 0 0 0 0 0 0 0 1</code></pre>
<p>To get easier interpretation of the prediction, we will convert the encoding into proper class label.</p>
<pre class="r"><code># Convert encoding to label
decode &lt;- function(x){
  case_when(x == 0 ~ &quot;cat&quot;,
            x == 1 ~ &quot;dog&quot;,
            x == 2 ~ &quot;panda&quot;
            )
}

pred_test &lt;- sapply(pred_test, decode) 

head(pred_test, 10)</code></pre>
<pre><code>#&gt;  [1] &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;dog&quot;</code></pre>
<p>Finally, we evaluate the model using the confusion matrix.</p>
<pre class="r"><code>confusionMatrix(as.factor(pred_test), 
                as.factor(df_test$class)
                )</code></pre>
<pre><code>#&gt; Confusion Matrix and Statistics
#&gt; 
#&gt;           Reference
#&gt; Prediction cat dog panda
#&gt;      cat    88  44     1
#&gt;      dog    22  58     6
#&gt;      panda   1   4   116
#&gt; 
#&gt; Overall Statistics
#&gt;                                               
#&gt;                Accuracy : 0.7706              
#&gt;                  95% CI : (0.7222, 0.8142)    
#&gt;     No Information Rate : 0.3618              
#&gt;     P-Value [Acc &gt; NIR] : &lt; 0.0000000000000002
#&gt;                                               
#&gt;                   Kappa : 0.6549              
#&gt;                                               
#&gt;  Mcnemar&#39;s Test P-Value : 0.05186             
#&gt; 
#&gt; Statistics by Class:
#&gt; 
#&gt;                      Class: cat Class: dog Class: panda
#&gt; Sensitivity              0.7928     0.5472       0.9431
#&gt; Specificity              0.8035     0.8803       0.9770
#&gt; Pos Pred Value           0.6617     0.6744       0.9587
#&gt; Neg Pred Value           0.8889     0.8110       0.9680
#&gt; Prevalence               0.3265     0.3118       0.3618
#&gt; Detection Rate           0.2588     0.1706       0.3412
#&gt; Detection Prevalence     0.3912     0.2529       0.3559
#&gt; Balanced Accuracy        0.7981     0.7138       0.9600</code></pre>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Many data comes in different forms and not only in structured format. Some data may come in form of text, image, or even video. That’s where the conventional machine learning model hit its limit and that’s where Deep Learning shines. Deep Learning can handle different unstructured data by simply adjusting the network architecture.</p>
</div>
